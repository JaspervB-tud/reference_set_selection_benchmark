{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "**NOTE**: we assume that the groundtruths are known since we base our results on simulations. In this, we assume that the user has access to a pair of files for every sample called `sample_sampleid_groundtruth_reads.tsv` and `sample_sampleid_groundtruth_folds` located in the corresponding sample folder. These files are provided on [Zenodo](https://doi.org/10.5281/zenodo.14727633) for the experiments in our manuscript and have for every line a species taxid followed by the relative abundance (either read-based or fold-based), delimited by a tab. For the read-based groundtruths a third column is added with the absolute number of reads of every species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_groundtruth_reads(sample_number):\n",
    "    abundances = {}\n",
    "    read_totals = {}\n",
    "    with open(f\"samples/sample_{sample_number}/sample_{sample_number}_groundtruth_reads.tsv\", \"r\") as f_in:\n",
    "        for line in f_in:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            abundances[int(line[0])] = float(line[1])\n",
    "            read_totals[int(line[0])] = int(line[2])\n",
    "    return abundances, read_totals\n",
    "\n",
    "def read_groundtruth_folds(sample_number):\n",
    "    abundances = {}\n",
    "    with open(f\"samples/sample_{sample_number}/sample_{sample_number}_groundtruth_folds.tsv\", \"r\") as f_in:\n",
    "        for line in f_in:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            abundances[int(line[0])] = float(line[1])\n",
    "    return abundances\n",
    "\n",
    "# Gather groundtruth for example sample_1\n",
    "groundtruth_reads, groundtruth_totals = read_groundtruth_reads(1)\n",
    "groundtruth_folds = read_groundtruth_folds(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kraken2 + Bracken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_bracken(method_threshold, sample_number, groundtruth_reads):\n",
    "    sample_species = list(groundtruth_reads.keys()) #all species taxids of species that er IN sample\n",
    "    relevant_species = set(sample_species)\n",
    "\n",
    "    metrics = {\"l1\": 0, \"f1\": 0, \"unclassified\": sum([groundtruth_reads[s] for s in groundtruth_reads])}\n",
    "    # F1-related metrics\n",
    "    tp = 0 \n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    predictions = {} #predicted abundance for every species\n",
    "    errors = {}\n",
    "    total_read_abundance = 0\n",
    "\n",
    "    with open(f\"estimations/sample_{sample_number}/kraken2-bracken/{method_threshold}.bracken\", \"r\") as f_in:\n",
    "        next(f_in) #skip header\n",
    "        for line in f_in:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            cur_taxid = int(line[1])\n",
    "            relevant_species.add(cur_taxid) #keep track of all species with abundance in predictions or groundtruth\n",
    "            cur_reads = int(line[5]) #number of reads assigned to cur_taxid\n",
    "            predictions[cur_taxid] = cur_reads\n",
    "            total_read_abundance += cur_reads\n",
    "            metrics[\"unclassified\"] -= cur_reads\n",
    "\n",
    "    # Filter results based on 0.1% minimum required abundance\n",
    "    filtered_total_read_abundance = 0\n",
    "    filtered_read_predictions = {}\n",
    "    for s in relevant_species:\n",
    "        try:\n",
    "            cur_abundance = predictions[s] / total_read_abundance\n",
    "            if cur_abundance >= 0.001:\n",
    "                filtered_read_predictions[s] = predictions[s]\n",
    "                filtered_total_read_abundance += predictions[s]\n",
    "            else:\n",
    "                filtered_read_predictions[s] = 0\n",
    "        except: #it can happen that some species in the sample is not estimated\n",
    "            cur_abundance = 0\n",
    "            filtered_read_predictions[s] = 0\n",
    "    \n",
    "    # Calculate errors\n",
    "    for s in relevant_species:\n",
    "        if s in sample_species:\n",
    "            errors[s] = abs( (filtered_read_predictions[s]/filtered_total_read_abundance) - groundtruth_reads[s] )\n",
    "            if filtered_read_predictions[s] > 0:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            errors[s] = abs( filtered_read_predictions[s]/filtered_total_read_abundance )\n",
    "            if filtered_read_predictions[s] > 0:\n",
    "                fp += 1\n",
    "        metrics[\"l1\"] += errors[s]\n",
    "\n",
    "    try:\n",
    "        precision = tp / (tp + fp)\n",
    "    except:\n",
    "        precision = 0\n",
    "    try:\n",
    "        recall = tp / (tp + fn)\n",
    "    except:\n",
    "        recall = 0\n",
    "    try:\n",
    "        metrics[\"f1\"] = 2 * ((precision * recall) / (precision + recall))\n",
    "    except:\n",
    "        metrics[\"f1\"] = 0\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centrifuge\n",
    "Note that for the Centrifuge results, we used the number of uniquely mapped reads to estimate relative abundances since the other results strongly deviated from the groundtruth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_centrifuge(method_threshold, sample_number, groundtruth_reads):\n",
    "    sample_species = list(groundtruth_reads.keys()) #all species taxids of species that er IN sample\n",
    "    relevant_species = set(sample_species)\n",
    "\n",
    "    metrics = {\"l1\": 0, \"f1\": 0} #unclassified reads for Centrifuge is not calculated here but is based on the .sam file\n",
    "    # F1-related metrics\n",
    "    tp = 0 \n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    predictions = {} #predicted abundance for every species\n",
    "    errors = {}\n",
    "    total_read_abundance = 0\n",
    "\n",
    "    with open(f\"estimations/sample_{sample_number}/centrifuge/{method_threshold}.report\", \"r\") as f_in:\n",
    "        next(f_in) #skip header\n",
    "        for line in f_in:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            cur_taxid = int(line[1])\n",
    "            relevant_species.add(cur_taxid) #keep track of all species with abundance in predictions or groundtruth\n",
    "            cur_level = line[2]\n",
    "            if cur_level == \"species\":\n",
    "                cur_reads = int(line[5]) #number of reads UNIQUELY assigned to cur_taxid\n",
    "                predictions[cur_taxid] = cur_reads\n",
    "                total_read_abundance += cur_reads\n",
    "\n",
    "    # Filter results based on 0.1% minimum required abundance\n",
    "    filtered_total_read_abundance = 0\n",
    "    filtered_read_predictions = {}\n",
    "    for s in relevant_species:\n",
    "        try:\n",
    "            cur_abundance = predictions[s] / total_read_abundance\n",
    "            if cur_abundance >= 0.001:\n",
    "                filtered_read_predictions[s] = predictions[s]\n",
    "                filtered_total_read_abundance += predictions[s]\n",
    "            else:\n",
    "                filtered_read_predictions[s] = 0\n",
    "        except: #it can happen that some species in the sample is not estimated\n",
    "            cur_abundance = 0\n",
    "            filtered_read_predictions[s] = 0\n",
    "    \n",
    "    # Calculate errors\n",
    "    for s in relevant_species:\n",
    "        if s in sample_species:\n",
    "            errors[s] = abs( (filtered_read_predictions[s]/filtered_total_read_abundance) - groundtruth_reads[s] )\n",
    "            if filtered_read_predictions[s] > 0:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            errors[s] = abs( filtered_read_predictions[s]/filtered_total_read_abundance )\n",
    "            if filtered_read_predictions[s] > 0:\n",
    "                fp += 1\n",
    "        metrics[\"l1\"] += errors[s]\n",
    "\n",
    "    try:\n",
    "        precision = tp / (tp + fp)\n",
    "    except:\n",
    "        precision = 0\n",
    "    try:\n",
    "        recall = tp / (tp + fn)\n",
    "    except:\n",
    "        recall = 0\n",
    "    try:\n",
    "        metrics[\"f1\"] = 2 * ((precision * recall) / (precision + recall))\n",
    "    except:\n",
    "        metrics[\"f1\"] = 0\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BWA + DUDes\n",
    "Note that DUDes estimates FOLD-based abundance (aka taxonomic abundance) rather than read-based abundance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dudes(method_threshold, sample_number, groundtruth_folds):\n",
    "    sample_species = list(groundtruth_reads.keys()) #all species taxids of species that er IN sample\n",
    "    relevant_species = set(sample_species)\n",
    "\n",
    "    metrics = {\"l1\": 0, \"f1\": 0} #unclassified reads for DUDes is not calculated here but is based on the .sam file produced by BWA\n",
    "    # F1-related metrics\n",
    "    tp = 0 \n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    predictions = {} #predicted abundance for every species\n",
    "    errors = {}\n",
    "    total_abundance = 0\n",
    "\n",
    "    with open(f\"estimations/sample_{sample_number}/bwa-dudes/{method_threshold}_dudes.out\", \"r\") as f_in:\n",
    "        for _ in range(6): #skip preamble\n",
    "            next(f_in)\n",
    "        for line in f_in:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            cur_taxid = int(line[0])\n",
    "            relevant_species.add(cur_taxid) #keep track of all species with abundance in predictions or groundtruth\n",
    "            cur_level = line[1]\n",
    "            if cur_level == \"species\":\n",
    "                cur_abundance = float(line[-1]) / 100\n",
    "                predictions[cur_taxid] = cur_abundance\n",
    "                total_abundance += cur_abundance\n",
    "\n",
    "    if total_fold_abundance > 0: #in the centroid reference sets, no estimates were generated at the species level\n",
    "        # Filter results based on 0.1% minimum required abundance\n",
    "        filtered_predictions = {}\n",
    "        filtered_total_abundance = 0\n",
    "        for s in relevant_species:\n",
    "            try:\n",
    "                cur_abundance = predictions[s] / total_abundance\n",
    "                if cur_abundance >= 0.001:\n",
    "                    filtered_predictions[s] = predictions[s]\n",
    "                    filtered_total_abundance += predictions[s]\n",
    "                else:\n",
    "                    filtered_predictions[s] = 0\n",
    "            except: #it can happen that some species in the sample is not estimated\n",
    "                cur_abundance = 0\n",
    "                filtered_predictions[s] = 0\n",
    "\n",
    "        # Calculate errors\n",
    "        for s in relevant_species:\n",
    "            if s in sample_species:\n",
    "                errors[s] = abs( (filtered_predictions[s]/filtered_total_abundance) - groundtruth_folds[s] )\n",
    "                if filtered_predictions[s] > 0:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fn += 1\n",
    "            else:\n",
    "                errors[s] = abs( (filtered_predictions[s]/filtered_total_abundance) )\n",
    "                if filtered_predictions[s] > 0:\n",
    "                    fp += 1\n",
    "            metrics[\"l1\"] += errors[s]\n",
    "    \n",
    "        try:\n",
    "            precision = tp / (tp + fp)\n",
    "        except:\n",
    "            precision = 0\n",
    "        try:\n",
    "            recall = tp / (tp + fn)\n",
    "        except:\n",
    "            recall = 0\n",
    "        try:\n",
    "            metrics[\"f1\"] = 2 * ((precision * recall) / (precision + recall))\n",
    "        except:\n",
    "            metrics[\"f1\"] = 0\n",
    "    else:\n",
    "        metrics[\"l1\"] = 2\n",
    "        metrics[\"f1\"] = 0\n",
    "\n",
    "    return metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark_processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
