{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"figures/bacteria\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "This fetches all of the species per experiment since selection was in principle ran for all bacterial species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genus_taxids = set()\n",
    "family_taxids = set()\n",
    "order_taxids = set()\n",
    "\n",
    "for experiment in [\"genus_experiments\", \"family_experiments\", \"order_experiments\"]:\n",
    "    filename = f\"root/reference_sets/{experiment}/all.tsv\"\n",
    "    with open(filename, \"r\") as f_in:\n",
    "        for line in f_in:\n",
    "            kingdom, species, _, _ = line.strip().split(\"\\t\")\n",
    "            if experiment == \"genus_experiments\":\n",
    "                genus_taxids.add(int(species))\n",
    "            elif experiment == \"family_experiments\":\n",
    "                family_taxids.add(int(species))\n",
    "            elif experiment == \"order_experiments\":\n",
    "                order_taxids.add(int(species))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MASH\n",
    "**NOTE**: here we assume that mash was run for every species individually, and timed using the `/usr/bin/time -v` command, saving runtimes in `root/runtimes/mash_${species}`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_runtimes_mash(genus_taxids, family_taxids, order_taxids):\n",
    "    times = {\n",
    "        \"total\": 0,\n",
    "        \"genus\": 0,\n",
    "        \"family\": 0,\n",
    "        \"order\": 0\n",
    "    }\n",
    "    memory = {\n",
    "        \"total\": 0,\n",
    "        \"genus\": 0,\n",
    "        \"family\": 0,\n",
    "        \"order\": 0\n",
    "    }\n",
    "    taxa = [int(f[len(\"mash_\"):]) for f in os.listdir(\"root/runtimes\") if f.startswith(\"mash_\")]\n",
    "    for species in taxa:\n",
    "        file = f\"root/runtimes/mash_{species}\"\n",
    "        with open(file, \"r\") as f_in:\n",
    "            cur_time = 0\n",
    "            cur_mem = 0\n",
    "            for line in f_in:\n",
    "                if \"User time (seconds)\" in line:\n",
    "                    cur_time += float(line.strip().split()[-1])\n",
    "                elif \"System time (seconds)\" in line:\n",
    "                    cur_time += float(line.strip().split()[-1])\n",
    "                elif \"Maximum resident set size (kbytes)\" in line:\n",
    "                    cur_mem = max(cur_mem, float(line.strip().split()[-1]) / (1024**2))  # Convert kbytes to GB\n",
    "        times[\"total\"] += cur_time\n",
    "        memory[\"total\"] = max(memory[\"total\"], cur_mem)\n",
    "        if species in genus_taxids: #store only for species in experiment\n",
    "            times[\"genus\"] += cur_time\n",
    "            memory[\"genus\"] = max(memory[\"genus\"], cur_mem)\n",
    "        if species in family_taxids: #store only for species in experiment\n",
    "            times[\"family\"] += cur_time\n",
    "            memory[\"family\"] = max(memory[\"family\"], cur_mem)\n",
    "        if species in order_taxids: #store only for species in experiment\n",
    "            times[\"order\"] += cur_time\n",
    "            memory[\"order\"] = max(memory[\"order\"], cur_mem)   \n",
    "\n",
    "    return times, memory           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection tools\n",
    "**NOTE**: here we assume that selection was run for every species individually, and timed using the `/usr/bin/time -v -a` command, saving runtimes collectively in `root/runtimes/${method}_${threshold}`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_runtimes_centroid(genus_taxids, family_taxids, order_taxids):\n",
    "    times = {\n",
    "        \"total\": 0,\n",
    "        \"genus\": 0,\n",
    "        \"family\": 0,\n",
    "        \"order\": 0\n",
    "    }\n",
    "    memory = {\n",
    "        \"total\": 0,\n",
    "        \"genus\": 0,\n",
    "        \"family\": 0,\n",
    "        \"order\": 0\n",
    "    }\n",
    "    runtime_per_taxid = {}\n",
    "    memory_per_taxid = {}\n",
    "    num_failed = 0\n",
    "\n",
    "    file = f\"root/runtimes/centroid\"\n",
    "    with open(file, \"r\") as f_in:\n",
    "        data = {}\n",
    "        for line in f_in:\n",
    "            if \"Command being timed\" in line:\n",
    "                data = {}\n",
    "                line = line.strip().split(\"taxid\")\n",
    "                data[\"taxid\"] = int(line[-1].split(\"\\\"\")[0])\n",
    "                cur_taxid = data[\"taxid\"]\n",
    "            elif \"User time (seconds)\" in line:\n",
    "                data[\"user time\"] = float(line.strip().split()[-1])\n",
    "            elif \"System time (seconds)\" in line:\n",
    "                data[\"system time\"] = float(line.strip().split()[-1])\n",
    "            elif \"Maximum resident set size\" in line: #note that this is in kbytes! -> divide by 1024^2\n",
    "                data[\"memory\"] = float(line.strip().split()[-1])/(1024**2)\n",
    "            elif \"Exit status\" in line:\n",
    "                data[\"exit status\"] = int(line.strip().split()[-1])\n",
    "                if data[\"exit status\"] == 0: #use latest available information\n",
    "                    runtime_per_taxid[cur_taxid] = data[\"user time\"] + data[\"system time\"]\n",
    "                    memory_per_taxid[cur_taxid] = data[\"memory\"]\n",
    "                else:\n",
    "                    num_failed += 1\n",
    "\n",
    "    for taxid in runtime_per_taxid:\n",
    "        times[\"total\"] += runtime_per_taxid[taxid]\n",
    "        memory[\"total\"] = max(memory[\"total\"], memory_per_taxid[taxid])\n",
    "        if taxid in genus_taxids:\n",
    "            times[\"genus\"] += runtime_per_taxid[taxid]\n",
    "            memory[\"genus\"] = max(memory[\"genus\"], memory_per_taxid[taxid])\n",
    "        if taxid in family_taxids:\n",
    "            times[\"family\"] += runtime_per_taxid[taxid]\n",
    "            memory[\"family\"] = max(memory[\"family\"], memory_per_taxid[taxid])\n",
    "        if taxid in order_taxids:\n",
    "            times[\"order\"] += runtime_per_taxid[taxid]\n",
    "            memory[\"order\"] = max(memory[\"order\"], memory_per_taxid[taxid])\n",
    "\n",
    "    return times, memory, num_failed\n",
    "\n",
    "def read_runtimes_clustering(genus_taxids, family_taxids, order_taxids):\n",
    "    # First process Single-linkage clustering\n",
    "    times_SL = {\n",
    "        \"total\": 0,\n",
    "        \"genus\": 0,\n",
    "        \"family\": 0,\n",
    "        \"order\": 0\n",
    "    }\n",
    "    memory_SL = {\n",
    "        \"total\": 0,\n",
    "        \"genus\": 0,\n",
    "        \"family\": 0,\n",
    "        \"order\": 0\n",
    "    }\n",
    "    runtime_per_taxid_SL = {}\n",
    "    memory_per_taxid_SL = {}\n",
    "    num_failed_SL = 0\n",
    "    file = f\"root/runtimes/single-linkage\" #threshold doesn't matter here since all refsets were generated at once\n",
    "    with open(file, \"r\") as f_in:\n",
    "        data = {}\n",
    "        for line in f_in:\n",
    "            if \"Command being timed\" in line:\n",
    "                data = {}\n",
    "                line = line.strip().split(\"taxid\")\n",
    "                data[\"taxid\"] = int(line[-1].split(\"--\")[0])\n",
    "                cur_taxid = data[\"taxid\"]\n",
    "            elif \"User time (seconds)\" in line:\n",
    "                data[\"user time\"] = float(line.strip().split()[-1])\n",
    "            elif \"System time (seconds)\" in line:\n",
    "                data[\"system time\"] = float(line.strip().split()[-1])\n",
    "            elif \"Maximum resident set size\" in line: #note that this is in kbytes! -> divide by 1024^2\n",
    "                data[\"memory\"] = float(line.strip().split()[-1])/(1024**2)\n",
    "            elif \"Exit status\" in line:\n",
    "                data[\"exit status\"] = int(line.strip().split()[-1])\n",
    "                if data[\"exit status\"] == 0: #use latest available information\n",
    "                    runtime_per_taxid_SL[cur_taxid] = data[\"user time\"] + data[\"system time\"]\n",
    "                    memory_per_taxid_SL[cur_taxid] = data[\"memory\"]\n",
    "                else:\n",
    "                    num_failed_SL += 1\n",
    "    for taxid in runtime_per_taxid_SL:\n",
    "        times_SL[\"total\"] += runtime_per_taxid_SL[taxid]\n",
    "        memory_SL[\"total\"] = max(memory_SL[\"total\"], memory_per_taxid_SL[taxid])\n",
    "        if taxid in genus_taxids:\n",
    "            times_SL[\"genus\"] += runtime_per_taxid_SL[taxid]\n",
    "            memory_SL[\"genus\"] = max(memory_SL[\"genus\"], memory_per_taxid_SL[taxid])\n",
    "        if taxid in family_taxids:\n",
    "            times_SL[\"family\"] += runtime_per_taxid_SL[taxid]\n",
    "            memory_SL[\"family\"] = max(memory_SL[\"family\"], memory_per_taxid_SL[taxid])\n",
    "        if taxid in order_taxids:\n",
    "            times_SL[\"order\"] += runtime_per_taxid_SL[taxid]\n",
    "            memory_SL[\"order\"] = max(memory_SL[\"order\"], memory_per_taxid_SL[taxid])\n",
    "\n",
    "    # Then process Complete-linkage clustering\n",
    "    times_CL = {\n",
    "        \"total\": 0,\n",
    "        \"genus\": 0,\n",
    "        \"family\": 0,\n",
    "        \"order\": 0\n",
    "    }\n",
    "    memory_CL = {\n",
    "        \"total\": 0,\n",
    "        \"genus\": 0,\n",
    "        \"family\": 0,\n",
    "        \"order\": 0\n",
    "    }\n",
    "    runtime_per_taxid_CL = {}\n",
    "    memory_per_taxid = {}\n",
    "    num_failed_CL = 0\n",
    "\n",
    "    file = f\"root/runtimes/single-linkage\"\n",
    "    with open(file, \"r\") as f_in:\n",
    "        data = {}\n",
    "        for line in f_in:\n",
    "            if \"Command being timed\" in line:\n",
    "                data = {}\n",
    "                line = line.strip().split(\"taxid\")\n",
    "                data[\"taxid\"] = int(line[-1].split(\"--\")[0])\n",
    "                cur_taxid = data[\"taxid\"]\n",
    "            elif \"User time (seconds)\" in line:\n",
    "                data[\"user time\"] = float(line.strip().split()[-1])\n",
    "            elif \"System time (seconds)\" in line:\n",
    "                data[\"system time\"] = float(line.strip().split()[-1])\n",
    "            elif \"Maximum resident set size\" in line: #note that this is in kbytes! -> divide by 1024^2\n",
    "                data[\"memory\"] = float(line.strip().split()[-1])/(1024**2)\n",
    "            elif \"Exit status\" in line:\n",
    "                data[\"exit status\"] = int(line.strip().split()[-1])\n",
    "                if data[\"exit status\"] == 0: #use latest available information\n",
    "                    runtime_per_taxid_CL[cur_taxid] = data[\"user time\"] + data[\"system time\"]\n",
    "                    memory_per_taxid[cur_taxid] = data[\"memory\"]\n",
    "                else:\n",
    "                    num_failed_CL += 1\n",
    "    for taxid in runtime_per_taxid_CL:\n",
    "        times_CL[\"total\"] += runtime_per_taxid_CL[taxid]\n",
    "        memory_CL[\"total\"] = max(memory_CL[\"total\"], memory_per_taxid[taxid])\n",
    "        if taxid in genus_taxids:\n",
    "            times_CL[\"genus\"] += runtime_per_taxid_CL[taxid]\n",
    "            memory_CL[\"genus\"] = max(memory_CL[\"genus\"], memory_per_taxid[taxid])\n",
    "        if taxid in family_taxids:\n",
    "            times_CL[\"family\"] += runtime_per_taxid_CL[taxid]\n",
    "            memory_CL[\"family\"] = max(memory_CL[\"family\"], memory_per_taxid[taxid])\n",
    "        if taxid in order_taxids:\n",
    "            times_CL[\"order\"] += runtime_per_taxid_CL[taxid]\n",
    "            memory_CL[\"order\"] = max(memory_CL[\"order\"], memory_per_taxid[taxid])\n",
    "\n",
    "    return times_SL, memory_SL, num_failed_SL, times_CL, memory_CL, num_failed_CL\n",
    "\n",
    "def read_runtimes_ggrasp(genus_taxids, family_taxids, order_taxids):\n",
    "    times = {\n",
    "        \"total\": 0,\n",
    "        \"genus\": 0,\n",
    "        \"family\": 0,\n",
    "        \"order\": 0\n",
    "    }\n",
    "    memory = {\n",
    "        \"total\": 0,\n",
    "        \"genus\": 0,\n",
    "        \"family\": 0,\n",
    "        \"order\": 0\n",
    "    }\n",
    "    num_failed = {\n",
    "        \"total\": 0,\n",
    "        \"genus\": 0,\n",
    "        \"family\": 0,\n",
    "        \"order\": 0\n",
    "    }\n",
    "    passed = set()\n",
    "    failed = set()\n",
    "    runtime_per_taxid = {}\n",
    "    memory_per_taxid = {}\n",
    "\n",
    "    file = f\"root/runtimes/GGRaSP\"\n",
    "    with open(file, \"r\") as f_in:\n",
    "        data = {}\n",
    "        for line in f_in:\n",
    "            if \"Command being timed\" in line:\n",
    "                data = {}\n",
    "                line = line.strip().split(f\"/tudelft.net/staff-umbrella/refsetbenchmark/species/2/\")\n",
    "                data[\"taxid\"] = int(line[-1].split(\":\")[0])\n",
    "                cur_taxid = data[\"taxid\"]\n",
    "            elif \"User time (seconds)\" in line:\n",
    "                data[\"user time\"] = float(line.strip().split()[-1])\n",
    "            elif \"System time (seconds)\" in line:\n",
    "                data[\"system time\"] = float(line.strip().split()[-1])\n",
    "            elif \"Maximum resident set size\" in line:\n",
    "                data[\"memory\"] = float(line.strip().split()[-1])/(1024**2)  # Convert kbytes to GB\n",
    "            elif \"Exit status\" in line:\n",
    "                data[\"exit status\"] = int(line.strip().split()[-1])\n",
    "                if data[\"exit status\"] == 0:# #use latest available information\n",
    "                    runtime_per_taxid[cur_taxid] = data[\"user time\"] + data[\"system time\"]\n",
    "                    memory_per_taxid[cur_taxid] = data[\"memory\"]\n",
    "                    passed.add(cur_taxid)\n",
    "                else:\n",
    "                    num_failed[\"total\"] += 1\n",
    "                    failed.add(cur_taxid)\n",
    "                    if cur_taxid in genus_taxids:\n",
    "                        num_failed[\"genus\"] += 1\n",
    "                    if cur_taxid in family_taxids:\n",
    "                        num_failed[\"family\"] += 1\n",
    "                    if cur_taxid in order_taxids:\n",
    "                        num_failed[\"order\"] += 1  \n",
    "\n",
    "    for taxid in passed:\n",
    "        if taxid in failed:\n",
    "            num_failed[\"total\"] -= 1\n",
    "            if taxid in genus_taxids:\n",
    "                num_failed[\"genus\"] -= 1\n",
    "            if taxid in family_taxids:\n",
    "                num_failed[\"family\"] -= 1\n",
    "            if taxid in order_taxids:\n",
    "                num_failed[\"order\"] -= 1 \n",
    "        times[\"total\"] += runtime_per_taxid[taxid]\n",
    "        memory[\"total\"] = max(memory[\"total\"], memory_per_taxid[taxid])\n",
    "        if taxid in genus_taxids:\n",
    "            times[\"genus\"] += runtime_per_taxid[taxid]\n",
    "            memory[\"genus\"] = max(memory[\"genus\"], memory_per_taxid[taxid])\n",
    "        if taxid in family_taxids:\n",
    "            times[\"family\"] += runtime_per_taxid[taxid]\n",
    "            memory[\"family\"] = max(memory[\"family\"], memory_per_taxid[taxid])\n",
    "        if taxid in order_taxids:\n",
    "            times[\"order\"] += runtime_per_taxid[taxid]\n",
    "            memory[\"order\"] = max(memory[\"order\"], memory_per_taxid[taxid])\n",
    "\n",
    "    return times, memory, num_failed\n",
    "\n",
    "def read_runtimes_gclust(threshold, genus_taxids, family_taxids, order_taxids):\n",
    "    times = {\n",
    "        \"total\": 0,\n",
    "        \"genus\": 0,\n",
    "        \"family\": 0,\n",
    "        \"order\": 0\n",
    "    }\n",
    "    memory = {\n",
    "        \"total\": 0,\n",
    "        \"genus\": 0,\n",
    "        \"family\": 0,\n",
    "        \"order\": 0\n",
    "    }\n",
    "    num_failed = {\n",
    "        \"total\": 0,\n",
    "        \"genus\": 0,\n",
    "        \"family\": 0,\n",
    "        \"order\": 0\n",
    "    }\n",
    "    passed = set()\n",
    "    failed = set()\n",
    "    runtime_per_taxid = {}\n",
    "    memory_per_taxid = {}\n",
    "\n",
    "    file = f\"root/runtimes/Gclust_{threshold}\"\n",
    "    with open(file, \"r\") as f_in:\n",
    "        data = {}\n",
    "        for line in f_in:\n",
    "            if \"Command being timed\" in line:\n",
    "                data = {}\n",
    "                line = line.strip().split(f\"root/genomes\")\n",
    "                data[\"taxid\"] = int(line[-1].split(\"/\")[0])\n",
    "                cur_taxid = data[\"taxid\"]\n",
    "            elif \"User time (seconds)\" in line:\n",
    "                data[\"user time\"] = float(line.strip().split()[-1])\n",
    "            elif \"System time (seconds)\" in line:\n",
    "                data[\"system time\"] = float(line.strip().split()[-1])\n",
    "            elif \"Maximum resident set size\" in line:\n",
    "                data[\"memory\"] = float(line.strip().split()[-1])/(1024**2)  # Convert kbytes to GB\n",
    "            elif \"Exit status\" in line:\n",
    "                data[\"exit status\"] = int(line.strip().split()[-1])\n",
    "                if data[\"exit status\"] == 0:#use latest available information\n",
    "                    runtime_per_taxid[cur_taxid] = data[\"user time\"] + data[\"system time\"]\n",
    "                    memory_per_taxid[cur_taxid] = data[\"memory\"]\n",
    "                    passed.add(cur_taxid)\n",
    "                else:\n",
    "                    num_failed[\"total\"] += 1\n",
    "                    failed.add(cur_taxid)\n",
    "                    if cur_taxid in genus_taxids:\n",
    "                        num_failed[\"genus\"] += 1\n",
    "                    if cur_taxid in family_taxids:\n",
    "                        num_failed[\"family\"] += 1\n",
    "                    if cur_taxid in order_taxids:\n",
    "                        num_failed[\"order\"] += 1\n",
    "\n",
    "    for taxid in passed:\n",
    "        if taxid in failed:\n",
    "            num_failed[\"total\"] -= 1\n",
    "            if taxid in genus_taxids:\n",
    "                num_failed[\"genus\"] -= 1\n",
    "            if taxid in family_taxids:\n",
    "                num_failed[\"family\"] -= 1\n",
    "            if taxid in order_taxids:\n",
    "                num_failed[\"order\"] -= 1 \n",
    "        times[\"total\"] += runtime_per_taxid[taxid]\n",
    "        memory[\"total\"] = max(memory[\"total\"], memory_per_taxid[taxid])\n",
    "        if taxid in genus_taxids:\n",
    "            times[\"genus\"] += runtime_per_taxid[taxid]\n",
    "            memory[\"genus\"] = max(memory[\"genus\"], memory_per_taxid[taxid])\n",
    "        if taxid in family_taxids:\n",
    "            times[\"family\"] += runtime_per_taxid[taxid]\n",
    "            memory[\"family\"] = max(memory[\"family\"], memory_per_taxid[taxid])\n",
    "        if taxid in order_taxids:\n",
    "            times[\"order\"] += runtime_per_taxid[taxid]\n",
    "            memory[\"order\"] = max(memory[\"order\"], memory_per_taxid[taxid])\n",
    "    \n",
    "    return times, memory, num_failed\n",
    "\n",
    "def read_runtimes_meshclust(threshold, genus_taxids, family_taxids, order_taxids):\n",
    "    times = {\n",
    "        \"total\": 0,\n",
    "        \"genus\": 0,\n",
    "        \"family\": 0,\n",
    "        \"order\": 0\n",
    "    }\n",
    "    memory = {\n",
    "        \"total\": 0,\n",
    "        \"genus\": 0,\n",
    "        \"family\": 0,\n",
    "        \"order\": 0\n",
    "    }\n",
    "    num_failed = {\n",
    "        \"total\": 0,\n",
    "        \"genus\": 0,\n",
    "        \"family\": 0,\n",
    "        \"order\": 0\n",
    "    }\n",
    "    passed = set()\n",
    "    failed = set()\n",
    "    runtime_per_taxid = {}\n",
    "    memory_per_taxid = {}\n",
    "\n",
    "    file = f\"root/runtimes/MeShClust_{threshold}\"\n",
    "    with open(file, \"r\") as f_in:\n",
    "        data = {}\n",
    "        for line in f_in:\n",
    "            if \"Command being timed\" in line:\n",
    "                data = {}\n",
    "                line = line.strip().split(f\"root/genomes\")\n",
    "                data[\"taxid\"] = int(line[-1].split(\"/\")[0])\n",
    "                cur_taxid = data[\"taxid\"]\n",
    "            elif \"User time (seconds)\" in line:\n",
    "                data[\"user time\"] = float(line.strip().split()[-1])\n",
    "            elif \"System time (seconds)\" in line:\n",
    "                data[\"system time\"] = float(line.strip().split()[-1])\n",
    "            elif \"Maximum resident set size\" in line:\n",
    "                data[\"memory\"] = float(line.strip().split()[-1])/(1024**2)  # Convert kbytes to GB\n",
    "            elif \"Exit status\" in line:\n",
    "                data[\"exit status\"] = int(line.strip().split()[-1])\n",
    "                if data[\"exit status\"] == 0:#use latest available information\n",
    "                    runtime_per_taxid[cur_taxid] = data[\"user time\"] + data[\"system time\"]\n",
    "                    memory_per_taxid[cur_taxid] = data[\"memory\"]\n",
    "                    passed.add(cur_taxid)\n",
    "                else:\n",
    "                    num_failed[\"total\"] += 1\n",
    "                    failed.add(cur_taxid)\n",
    "                    if cur_taxid in genus_taxids:\n",
    "                        num_failed[\"genus\"] += 1\n",
    "                    if cur_taxid in family_taxids:\n",
    "                        num_failed[\"family\"] += 1\n",
    "                    if cur_taxid in order_taxids:\n",
    "                        num_failed[\"order\"] += 1  \n",
    "\n",
    "    for taxid in passed:\n",
    "        if taxid in failed:\n",
    "            num_failed[\"total\"] -= 1\n",
    "            if taxid in genus_taxids:\n",
    "                num_failed[\"genus\"] -= 1\n",
    "            if taxid in family_taxids:\n",
    "                num_failed[\"family\"] -= 1\n",
    "            if taxid in order_taxids:\n",
    "                num_failed[\"order\"] -= 1 \n",
    "        times[\"total\"] += runtime_per_taxid[taxid]\n",
    "        memory[\"total\"] = max(memory[\"total\"], memory_per_taxid[taxid])\n",
    "        if taxid in genus_taxids:\n",
    "            times[\"genus\"] += runtime_per_taxid[taxid]\n",
    "            memory[\"genus\"] = max(memory[\"genus\"], memory_per_taxid[taxid])\n",
    "        if taxid in family_taxids:\n",
    "            times[\"family\"] += runtime_per_taxid[taxid]\n",
    "            memory[\"family\"] = max(memory[\"family\"], memory_per_taxid[taxid])\n",
    "        if taxid in order_taxids:\n",
    "            times[\"order\"] += runtime_per_taxid[taxid]\n",
    "            memory[\"order\"] = max(memory[\"order\"], memory_per_taxid[taxid])\n",
    "\n",
    "    return times, memory, num_failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index building\n",
    "**NOTE**: here we assume that index building was timed using the `/usr/bin/time -v` command, saving runtimes collectively in `root/runtimes/indexing_${experiment}_${profiler}_${method}_${threshold}`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_runtimes_kraken_indexing(experiment, method):\n",
    "    file_path = f\"root/runtimes/indexing_{experiment}_kraken_{method}\"\n",
    "    time = 0\n",
    "    memory = 0\n",
    "    with open(file_path, \"r\") as f_in:\n",
    "        data = {\"time\": 0, \"memory\": 0}\n",
    "        for line in f_in:\n",
    "            if \"User time (seconds)\" in line:\n",
    "                data[\"time\"] += float(line.strip().split()[-1])\n",
    "            elif \"System time (seconds)\" in line:\n",
    "                data[\"time\"] += float(line.strip().split()[-1])\n",
    "            elif \"Maximum resident set size (kbytes)\" in line:\n",
    "                data[\"memory\"] = int(line.strip().split()[-1]) / (1024**2)\n",
    "            elif \"Exit status\" in line:\n",
    "                if int(line.strip().split()[-1]) == 0:\n",
    "                    time += data[\"time\"]\n",
    "                    memory = max(memory, data[\"memory\"])\n",
    "                data = {\"time\": 0, \"memory\": 0}\n",
    "\n",
    "    return time, memory\n",
    "\n",
    "def read_runtimes_centrifuge_indexing(experiment, method):\n",
    "    file_path = f\"root/runtimes/indexing_{experiment}_centrifuge_{method}\"\n",
    "    time = 0\n",
    "    memory = 0\n",
    "    with open(file_path, \"r\") as f_in:\n",
    "        data = {\"time\": 0, \"memory\": 0}\n",
    "        for line in f_in:\n",
    "            if \"User time (seconds)\" in line:\n",
    "                data[\"time\"] += float(line.strip().split()[-1])\n",
    "            elif \"System time (seconds)\" in line:\n",
    "                data[\"time\"] += float(line.strip().split()[-1])\n",
    "            elif \"Maximum resident set size (kbytes)\" in line:\n",
    "                data[\"memory\"] = int(line.strip().split()[-1]) / (1024**2)\n",
    "            elif \"Exit status\" in line:\n",
    "                if int(line.strip().split()[-1]) == 0:\n",
    "                    time += data[\"time\"]\n",
    "                    memory = max(memory, data[\"memory\"])\n",
    "                data = {\"time\": 0, \"memory\": 0}\n",
    "\n",
    "    return time, memory\n",
    "\n",
    "def read_runtimes_dudes_indexing(experiment, method):\n",
    "    file_path = f\"root/runtimes/indexing_{experiment}_dudes_{method}\"\n",
    "    time = 0\n",
    "    memory = 0\n",
    "    with open(file_path, \"r\") as f_in:\n",
    "        data = {\"time\": 0, \"memory\": 0}\n",
    "        for line in f_in:\n",
    "            if \"User time (seconds)\" in line:\n",
    "                data[\"time\"] += float(line.strip().split()[-1])\n",
    "            elif \"System time (seconds)\" in line:\n",
    "                data[\"time\"] += float(line.strip().split()[-1])\n",
    "            elif \"Maximum resident set size (kbytes)\" in line:\n",
    "                data[\"memory\"] = int(line.strip().split()[-1]) / (1024**2)\n",
    "            elif \"Exit status\" in line:\n",
    "                if int(line.strip().split()[-1]) == 0:\n",
    "                    time += data[\"time\"]\n",
    "                    memory = max(memory, data[\"memory\"])\n",
    "                data = {\"time\": 0, \"memory\": 0}\n",
    "\n",
    "    return time, memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling\n",
    "**NOTE**: here we assume that profiling was timed using the `/usr/bin/time -v -a` command, saving runtimes collectively (over all samples) in `root/runtimes/profiling_${experiment}_${profiler}_${method}_${threshold}`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_runtimes_kraken_profiling(experiment, method):\n",
    "    file_path = f\"root/runtimes/profiling_{experiment}_kraken_{method}\"\n",
    "    times = []\n",
    "    memory = 0\n",
    "    with open(file_path, \"r\") as f_in:\n",
    "        data = {\"time\": 0, \"memory\": 0}\n",
    "        cur_command = None\n",
    "        for line in f_in:\n",
    "            if 'Command being timed:' in line:\n",
    "                if \"kraken2 --db\" in line:\n",
    "                    cur_command = \"kraken\"\n",
    "                else:\n",
    "                    cur_command = \"bracken\"\n",
    "            if \"User time (seconds)\" in line:\n",
    "                data[\"time\"] += float(line.strip().split()[-1])\n",
    "            elif \"System time (seconds)\" in line:\n",
    "                data[\"time\"] += float(line.strip().split()[-1])\n",
    "            elif \"Maximum resident set size (kbytes)\" in line:\n",
    "                data[\"memory\"] = max(data[\"memory\"], int(line.strip().split()[-1]) / (1024**2))\n",
    "            elif \"Exit status\" in line and cur_command == \"bracken\": #store collective time and memory for kraken+bracken\n",
    "                if int(line.strip().split()[-1]) == 0:\n",
    "                    times.append(data[\"time\"])\n",
    "                    memory = max(memory, data[\"memory\"])\n",
    "                data = {\"time\": 0, \"memory\": 0}\n",
    "                cur_command = None\n",
    "\n",
    "    return times, memory\n",
    "\n",
    "def read_runtimes_centrifuge_profiling(experiment, method):\n",
    "    file_path = f\"root/runtimes/profiling_{experiment}_centrifuge_{method}\"\n",
    "    times = []\n",
    "    memory = 0\n",
    "    with open(file_path, \"r\") as f_in:\n",
    "        data = {\"time\": 0, \"memory\": 0}\n",
    "        for line in f_in:\n",
    "            if \"User time (seconds)\" in line:\n",
    "                data[\"time\"] += float(line.strip().split()[-1])\n",
    "            elif \"System time (seconds)\" in line:\n",
    "                data[\"time\"] += float(line.strip().split()[-1])\n",
    "            elif \"Maximum resident set size (kbytes)\" in line:\n",
    "                data[\"memory\"] = int(line.strip().split()[-1]) / (1024**2)\n",
    "            elif \"Exit status\" in line:\n",
    "                if int(line.strip().split()[-1]) == 0:\n",
    "                    times.append(data[\"time\"])\n",
    "                    memory = max(memory, data[\"memory\"])\n",
    "                data = {\"time\": 0, \"memory\": 0}\n",
    "\n",
    "    return times, memory\n",
    "\n",
    "def read_runtimes_dudes_profiling(experiment, method):\n",
    "    file_path = f\"root/runtimes/profiling_{experiment}_dudes_{method}\"\n",
    "    times = []\n",
    "    memory = 0\n",
    "    with open(file_path, \"r\") as f_in:\n",
    "        data = {\"time\": 0, \"memory\": 0}\n",
    "        cur_command = False\n",
    "        for line in f_in:\n",
    "            if \"Command being timed:\" in line:\n",
    "                if \"dudes -s\" in line and \"-l species\" in line:\n",
    "                    cur_command = True #after the dudes command, we have total profiling time and memory (BWA + DUDes)\n",
    "            if \"User time (seconds)\" in line:\n",
    "                data[\"time\"] += float(line.strip().split()[-1])\n",
    "            elif \"System time (seconds)\" in line:\n",
    "                data[\"time\"] += float(line.strip().split()[-1])\n",
    "            elif \"Maximum resident set size (kbytes)\" in line:\n",
    "                data[\"memory\"] = max(data[\"memory\"], int(line.strip().split()[-1]) / (1024**2))\n",
    "            elif \"Exit status\" in line and cur_command:\n",
    "                if int(line.strip().split()[-1]) == 0:\n",
    "                    times.append(data[\"time\"])\n",
    "                    memory = max(memory, data[\"memory\"])\n",
    "                data = {\"time\": 0, \"memory\": 0}\n",
    "                cur_command = False\n",
    "\n",
    "    return times, memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch runtimes and memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First determine proportions of runtimes (i.e. how much of total time is spent on steps)\n",
    "mash_steps = [\n",
    "    \"centroid\",\n",
    "    \"ggrasp\",\n",
    "    \"single-linkage\",\n",
    "    \"complete-linkage\"\n",
    "]\n",
    "\n",
    "METHODS = [\"all\", \"centroid\", \"ggrasp\", \"meshclust_0.95\", \"meshclust_0.97\", \"meshclust_0.99\", \"gclust_0.95\", \"gclust_0.97\", \"gclust_0.99\", \"single-linkage_0.95\", \"single-linkage_0.97\", \"single-linkage_0.99\", \"complete-linkage_0.95\", \"complete-linkage_0.97\", \"complete-linkage_0.99\"]\n",
    "METHOD_LABELS = {\n",
    "    \"all\": \"All\",\n",
    "    \"centroid\": \"Centroid\",\n",
    "    \"ggrasp\": \"GGRaSP\",\n",
    "    \"meshclust_0.95\": \"MC-0.95\",\n",
    "    \"meshclust_0.97\": \"MC-0.97\",\n",
    "    \"meshclust_0.99\": \"MC-0.99\",\n",
    "    \"gclust_0.95\": \"GC-0.95\",\n",
    "    \"gclust_0.97\": \"GC-0.97\",\n",
    "    \"gclust_0.99\": \"GC-0.99\",\n",
    "    \"single-linkage_0.95\": \"SL-0.95\",\n",
    "    \"single-linkage_0.97\": \"SL-0.97\",\n",
    "    \"single-linkage_0.99\": \"SL-0.99\",\n",
    "    \"complete-linkage_0.95\": \"CL-0.95\",\n",
    "    \"complete-linkage_0.97\": \"CL-0.97\",\n",
    "    \"complete-linkage_0.99\": \"CL-0.99\"\n",
    "}\n",
    "EXPERIMENTS = [\"genus_experiments\", \"family_experiments\", \"order_experiments\"]\n",
    "\n",
    "\n",
    "compared_to_all = []\n",
    "for profiler in [\"kraken\", \"centrifuge\", \"dudes\"]:\n",
    "    times_selection = {}\n",
    "    memory_selection = {}\n",
    "    num_failed = {}\n",
    "    # Calculate runtimes for selection\n",
    "    times_selection[\"mash\"], memory_selection[\"mash\"] = read_runtimes_mash(genus_taxids, family_taxids, order_taxids)\n",
    "    times_selection[\"centroid\"], memory_selection[\"centroid\"], num_failed[\"centroid\"] = read_runtimes_centroid(genus_taxids, family_taxids, order_taxids)\n",
    "    times_selection[\"single-linkage\"], memory_selection[\"single-linkage\"], num_failed[\"single-linkage\"], times_selection[\"complete-linkage\"], memory_selection[\"complete-linkage\"], num_failed[\"complete-linkage\"] = read_runtimes_clustering(genus_taxids, family_taxids, order_taxids)\n",
    "    times_selection[\"ggrasp\"], memory_selection[\"ggrasp\"], num_failed[\"ggrasp\"] = read_runtimes_ggrasp(genus_taxids, family_taxids, order_taxids)\n",
    "    times_selection[\"gclust_0.95\"], memory_selection[\"gclust_0.95\"], num_failed[\"gclust_0.95\"] = read_runtimes_gclust(95, genus_taxids, family_taxids, order_taxids)\n",
    "    times_selection[\"gclust_0.97\"], memory_selection[\"gclust_0.97\"], num_failed[\"gclust_0.97\"] = read_runtimes_gclust(97, genus_taxids, family_taxids, order_taxids)\n",
    "    times_selection[\"gclust_0.99\"], memory_selection[\"gclust_0.99\"], num_failed[\"gclust_0.99\"] = read_runtimes_gclust(99, genus_taxids, family_taxids, order_taxids)\n",
    "    times_selection[\"meshclust_0.95\"], memory_selection[\"meshclust_0.95\"], num_failed[\"meshclust_0.95\"] = read_runtimes_meshclust(95, genus_taxids, family_taxids, order_taxids)\n",
    "    times_selection[\"meshclust_0.97\"], memory_selection[\"meshclust_0.97\"], num_failed[\"meshclust_0.97\"] = read_runtimes_meshclust(97, genus_taxids, family_taxids, order_taxids)\n",
    "    times_selection[\"meshclust_0.99\"], memory_selection[\"meshclust_0.99\"], num_failed[\"meshclust_0.99\"] = read_runtimes_meshclust(99, genus_taxids, family_taxids, order_taxids)\n",
    "\n",
    "    times_indexing = {}\n",
    "    memory_indexing = {}\n",
    "    # Calculate runtimes for indexing\n",
    "    for experiment in tqdm(EXPERIMENTS):\n",
    "        times_indexing[experiment] = {}\n",
    "        memory_indexing[experiment] = {}\n",
    "        for method in METHODS:\n",
    "            if profiler == \"kraken\":\n",
    "                times_indexing[experiment][method], memory_indexing[experiment][method] = read_runtimes_kraken_indexing(experiment, method)\n",
    "            elif profiler == \"centrifuge\":\n",
    "                times_indexing[experiment][method], memory_indexing[experiment][method] = read_runtimes_centrifuge_indexing(experiment, method)\n",
    "            elif profiler == \"dudes\":\n",
    "                times_indexing[experiment][method], memory_indexing[experiment][method] = read_runtimes_dudes_indexing(experiment, method)\n",
    "\n",
    "    times_profiling = {}\n",
    "    memory_profiling = {}\n",
    "    # Calculate runtimes for profiling\n",
    "    for experiment in tqdm(EXPERIMENTS):\n",
    "        times_profiling[experiment] = {}\n",
    "        memory_profiling[experiment] = {}\n",
    "        for method in METHODS:\n",
    "            if profiler == \"kraken\":\n",
    "                times_profiling[experiment][method], memory_profiling[experiment][method] = read_runtimes_kraken_profiling(experiment, method)\n",
    "            elif profiler == \"centrifuge\":\n",
    "                times_profiling[experiment][method], memory_profiling[experiment][method] = read_runtimes_centrifuge_profiling(experiment, method)\n",
    "            elif profiler == \"dudes\":\n",
    "                times_profiling[experiment][method], memory_profiling[experiment][method] = read_runtimes_dudes_profiling(experiment, method)\n",
    "            times_profiling[experiment][method] = np.mean(times_profiling[experiment][method])\n",
    "\n",
    "    for experiment in EXPERIMENTS:\n",
    "        level = experiment.split(\"_\")[0]\n",
    "        print(profiler, experiment)\n",
    "        for method in METHODS:\n",
    "            # selection\n",
    "            if method in mash_steps:\n",
    "                total_selection = times_selection[\"mash\"][level]\n",
    "                total_selection_mem = memory_selection[\"mash\"][level]\n",
    "            else:\n",
    "                total_selection = 0\n",
    "                total_selection_mem = 0\n",
    "            try:\n",
    "                total_selection += times_selection[method][level]\n",
    "                total_selection_mem = max(total_selection_mem, memory_selection[method][level])\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    total_selection += times_selection['-'.join(method.split(\"-\")[:-1])][level]\n",
    "                    total_selection_mem = max(total_selection_mem, memory_selection['-'.join(method.split(\"-\")[:-1])][level])\n",
    "                except: #all selection does not have a selection time\n",
    "                    total_selection += 0\n",
    "                    total_selection_mem = max(total_selection_mem, 0)\n",
    "            # indexing\n",
    "            total_indexing = times_indexing[experiment][method]\n",
    "            total_indexing_mem = memory_indexing[experiment][method]\n",
    "\n",
    "            # profiling (average)\n",
    "            total_profiling = times_profiling[experiment][method]\n",
    "            total_profiling_mem = memory_profiling[experiment][method]\n",
    "\n",
    "            #total_time = total_selection + total_indexing + total_profiling #this includes profiling\n",
    "            total_time = total_selection + total_indexing  #this excludes profiling\n",
    "            compared_to_all.append({\n",
    "                \"profiler\": profiler,\n",
    "                \"experiment\": experiment,\n",
    "                \"method\": method,\n",
    "                \"threshold\": method.split(\"-\")[-1] if len(method.split(\"-\")) > 1 else \"-\",\n",
    "                \"total_time\": total_time,\n",
    "                \"profiling_time\": total_profiling,\n",
    "                #\"memory\": max(total_selection_mem, total_indexing_mem, total_profiling_mem),\n",
    "                \"memory\": max(total_selection_mem, total_indexing_mem),\n",
    "                \"profiling_memory\": total_profiling_mem,\n",
    "                \"relative_to_all\": total_time / (times_profiling[experiment][\"all\"] + times_indexing[experiment][\"all\"]),\n",
    "                \"mem_relative_to_all\": max(total_selection_mem, total_indexing_mem, total_profiling_mem) / max(memory_indexing[experiment][\"all\"], memory_profiling[experiment][\"all\"]),\n",
    "            })\n",
    "            print(f\"Method={method}, selection={total_selection/total_time*100:.2f}%, indexing={total_indexing/total_time*100:.2f}%, profiling={total_profiling/total_time*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_methods = [\n",
    "    \"centroid\",\n",
    "    \"ggrasp\",\n",
    "    \"single-linkage_0.95\",\n",
    "    \"single-linkage_0.97\",\n",
    "    \"single-linkage_0.99\",\n",
    "    \"complete-linkage_0.95\",\n",
    "    \"complete-linkage_0.97\",\n",
    "    \"complete-linkage_0.99\",\n",
    "]\n",
    "meshclust_methods = [\n",
    "    \"meshclust_0.95\",\n",
    "    \"meshclust_0.97\",\n",
    "    \"meshclust_0.99\",\n",
    "]\n",
    "gclust_methods = [\n",
    "    \"gclust_0.95\",\n",
    "    \"gclust_0.97\",\n",
    "    \"gclust_0.99\",\n",
    "]\n",
    "method_to_group = {}\n",
    "for method in clustering_methods:\n",
    "    method_to_group[method] = \"Hierarchical\\nclustering\"\n",
    "for method in meshclust_methods:\n",
    "    method_to_group[method] = \"MeShClust\"\n",
    "for method in gclust_methods:\n",
    "    method_to_group[method] = \"Gclust\"\n",
    "method_to_group[\"all\"] = \"All\"\n",
    "\n",
    "# Create copy and add column for method groups\n",
    "df = pd.DataFrame(compared_to_all)\n",
    "df_copy = df.copy()\n",
    "if \"method_group\" not in df_copy.columns:\n",
    "    df_copy = df_copy.assign(method_group=df_copy[\"method\"].map(method_to_group))\n",
    "df_copy.rename(columns={\"method_group\": \"Method group\"}, inplace=True)\n",
    "#df_copy[\"relative_to_all\"] *= 100\n",
    "#df_copy[\"mem_relative_to_all\"] *= 100\n",
    "print(df_copy.columns)\n",
    "\n",
    "group_order = [\"Hierarchical\\nclustering\", \"MeShClust\", \"Gclust\", \"All\"]\n",
    "exp_order = [\"genus_experiments\", \"family_experiments\", \"order_experiments\"]\n",
    "for experiment in exp_order:\n",
    "    level = experiment.split(\"_\")[0]\n",
    "    cur_df = df_copy[df_copy[\"experiment\"] == experiment]\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    ax = sns.boxplot(\n",
    "        data=cur_df,\n",
    "        x=\"Method group\",\n",
    "        y=\"profiling_time\",\n",
    "        order=group_order,\n",
    "        dodge=True,\n",
    "        width=0.8,\n",
    "        gap=0.05,\n",
    "        patch_artist=True,\n",
    "        boxprops=dict(facecolor='silver', edgecolor=\"black\", alpha=0.6),\n",
    "        medianprops=dict(color='black'),\n",
    "        whiskerprops=dict(color='black'),\n",
    "        capprops=dict(color='black'),\n",
    "        flierprops=dict(marker=''),\n",
    "    )\n",
    "    sns.stripplot(\n",
    "        data=cur_df,\n",
    "        x=\"Method group\",\n",
    "        y=\"profiling_time\",\n",
    "        hue=\"profiler\",\n",
    "        order=group_order,\n",
    "        size=10,\n",
    "        alpha=0.75,\n",
    "        ax=ax,\n",
    "        dodge=False,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=1,\n",
    "        jitter=0.2,\n",
    "    )\n",
    "    \n",
    "    plt.title(experiment, fontsize=20)\n",
    "    plt.yticks(fontsize=13)\n",
    "    plt.ylabel(\"\", fontsize=16)\n",
    "    plt.ylim(bottom=min(df_copy[\"profiling_time\"].min()*0.8, 1), top=10**np.ceil(np.log10(df_copy[\"profiling_time\"].max())))\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.xlabel(\"\")\n",
    "    ax.set_yscale(\"log\", base=10)\n",
    "    plt.tight_layout()\n",
    "    ax.grid(axis=\"y\", which=\"both\", linestyle=\"--\", linewidth=0.5, alpha=0.3)\n",
    "    ax.grid(axis=\"y\", which=\"major\", linestyle=\"--\", linewidth=1.5, alpha=0.8)\n",
    "    # This creates the plots in Figure 7\n",
    "    fig.savefig(f\"{OUTPUT_DIR}/{experiment}/runtimes_profiling.svg\", dpi=500, bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = sns.boxplot(\n",
    "        data=cur_df,\n",
    "        x=\"Method group\",\n",
    "        y=\"profiling_memory\",\n",
    "        order=group_order,\n",
    "        dodge=True,\n",
    "        width=0.8,\n",
    "        gap=0.05,\n",
    "        patch_artist=True,\n",
    "        boxprops=dict(facecolor='silver', edgecolor=\"black\", alpha=0.6),\n",
    "        medianprops=dict(color='black'),\n",
    "        whiskerprops=dict(color='black'),\n",
    "        capprops=dict(color='black'),\n",
    "        flierprops=dict(marker=''),\n",
    "    )\n",
    "    sns.stripplot(\n",
    "        data=cur_df,\n",
    "        x=\"Method group\",\n",
    "        y=\"profiling_memory\",\n",
    "        hue=\"profiler\",\n",
    "        order=group_order,\n",
    "        size=10,\n",
    "        alpha=0.75,\n",
    "        ax=ax,\n",
    "        dodge=False,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=1,\n",
    "        jitter=0.2,\n",
    "    )\n",
    "    plt.title(experiment, fontsize=20)\n",
    "    plt.yticks(fontsize=13)\n",
    "    plt.ylabel(\"\", fontsize=16)\n",
    "    plt.ylim(bottom=min(df_copy[\"profiling_memory\"].min()*0.8, 1), top=10**np.ceil(np.log10(df_copy[\"profiling_memory\"].max())))\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.xlabel(\"\")\n",
    "    ax.set_yscale(\"log\", base=10)\n",
    "    plt.tight_layout()\n",
    "    ax.grid(axis=\"y\", which=\"both\", linestyle=\"--\", linewidth=0.5, alpha=0.3)\n",
    "    ax.grid(axis=\"y\", which=\"major\", linestyle=\"--\", linewidth=1.5, alpha=0.8)\n",
    "    # This creates the plots in Supplementary Figure S8\n",
    "    fig.savefig(f\"{OUTPUT_DIR}/{experiment}/memory_profiling.svg\", dpi=500, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
